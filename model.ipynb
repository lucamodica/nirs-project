{"cells":[{"cell_type":"markdown","metadata":{},"source":["# NIRS (model)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import utils as utils\n","import torch\n","import numpy as np\n","import pandas as pd\n","# from icecream import ic\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import sklearn\n","import random\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","\n","    np.random.seed(seed)\n","\n","    sklearn.utils.check_random_state(seed)\n","\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","    pd.set_option('display.max_columns', None)\n","    pd.set_option('display.max_rows', None)\n","    pd.set_option('display.width', None)\n","    pd.set_option('display.expand_frame_repr', False)\n","\n","seed_everything(42)\n","\n","%matplotlib inline\n","sns.set_style('darkgrid')\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install icecream torchsummary"]},{"cell_type":"markdown","metadata":{},"source":["## Init and reading data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["products_df = pd.read_csv('/kaggle/input/dataaa/products_sampled_processed.csv')\n","reviews_df = pd.read_csv('/kaggle/input/dataaa/reviews_sampled_processed.csv')\n","\n","print(products_df.shape, reviews_df.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["reviews_df.drop_duplicates(subset=['reviewerID', 'asin', 'reviewText', 'summary'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = reviews_df.merge(products_df, on='asin', how='inner')\n","\n","unreviewed_products_df = products_df[~products_df['asin'].isin(reviews_df['asin'])]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(f'Shape of df: {df.shape}')\n","print(f'Shape of unreviewed_products_df: {unreviewed_products_df.shape}')\n","print(f'Shape of products_df: {products_df.shape}')\n","print(f'Shape of reviews_df: {reviews_df.shape}')"]},{"cell_type":"markdown","metadata":{},"source":["## Other data preparation for the model"]},{"cell_type":"markdown","metadata":{},"source":["### User and product id mapping"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["user_ids = df['reviewerID'].values\n","product_ids = products_df['asin'].values\n","\n","user_id_map = {uid: idx for idx, uid in enumerate(set(user_ids))}\n","product_id_map = {pid: idx for idx, pid in enumerate(set(product_ids))}\n","\n","# Map user and item IDs to indices\n","df['user_index'] = df['reviewerID'].map(user_id_map)\n","df['item_index'] = df['asin'].map(product_id_map)\n","products_df['item_index'] = products_df['asin'].map(product_id_map)"]},{"cell_type":"markdown","metadata":{},"source":["### Text Embeddings (with Word2Vec or whatever)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from gensim.models import Word2Vec\n","\n","def create_word2vec_embeddings(texts, embedding_dim):\n","  tokenized_texts = [text.split() for text in texts]\n","  \n","  model = Word2Vec(tokenized_texts, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n","#   model.wv.gpu = True\n","  \n","  embeddings = []\n","  for text in tokenized_texts:\n","      embedding = np.mean([model.wv[word] for word in text if word in model.wv], axis=0)\n","      embeddings.append(embedding)\n","  \n","  return np.array(embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_reviewed_products = products_df[products_df['asin'].isin(df['asin'].unique())].drop_duplicates(subset=['asin'])\n","df_reviewed_products['brand'] = df_reviewed_products['brand'].astype(str)\n","df_reviewed_products['title'] = df_reviewed_products['title'].astype(str)\n","df_reviewed_products['description'] = df_reviewed_products['description'].astype(str)\n","\n","df_reviewed_products.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%time\n","\n","text_embedding_dims = 200\n","reviewTexts_embs = torch.tensor(create_word2vec_embeddings(reviews_df['reviewText'].astype(str), embedding_dim=text_embedding_dims))\n","summary_embs = torch.tensor(create_word2vec_embeddings(reviews_df['summary'].astype(str), embedding_dim=text_embedding_dims))\n","title_embs = torch.tensor(create_word2vec_embeddings(products_df['title'].astype(str), embedding_dim=text_embedding_dims))\n","description_embs = torch.tensor(create_word2vec_embeddings(products_df['description'].astype(str), embedding_dim=text_embedding_dims))\n","feature_embs = torch.tensor(create_word2vec_embeddings(products_df['feature'].astype(str), embedding_dim=text_embedding_dims))\n","brand_embs = torch.tensor(create_word2vec_embeddings(products_df['brand'].astype(str), embedding_dim=text_embedding_dims))\n","\n","#load the embeddings  \n","# reviewTexts_embs = torch.load('data/embeds/review_embeddings.pt')\n","# summary_embs = torch.load('data/embeds/summary_embeddings.pt')\n","# description_embs = torch.load('data/embeds/description_embeddings.pt')\n","# title_embs = torch.load('data/embeds/title_embeddings.pt')\n","# feature_embs = torch.load('data/embeds/feature_embeddings.pt')\n","# brand_embs = torch.load('data/embeds/brand_embeddings.pt')\n","\n","print(reviewTexts_embs.shape, summary_embs.shape, title_embs.shape, description_embs.shape, feature_embs.shape, brand_embs.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["reviews_text_embs = torch.mean(torch.stack([reviewTexts_embs, summary_embs]), dim=0)\n","products_text_embs = torch.mean(torch.stack([title_embs, description_embs, feature_embs, brand_embs]), dim=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["user_embeddings_map = {}\n","for i, row in reviews_df.iterrows():\n","    user_id = row['reviewerID']   \n","    if user_id not in user_embeddings_map:\n","        user_embeddings_map[user_id] = [reviews_text_embs[i]]\n","    else:\n","        user_embeddings_map[user_id].append(reviews_text_embs[i])\n","\n","for user_id, emb in user_embeddings_map.items():\n","    user_tensors = user_embeddings_map[user_id]\n","    user_embeddings_map[user_id] = torch.mean(torch.stack(user_tensors), dim=0)\n","    \n","    \n","df['user_embs'] = df['reviewerID'].map(user_embeddings_map)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["all_product_embeddings_map = {}\n","for i, row in products_df.iterrows():\n","    product_id = row['asin']\n","    all_product_embeddings_map[product_id] = products_text_embs[i]\n","products_df['product_embs'] = products_df['asin'].map(all_product_embeddings_map)\n","\n","reviewed_products_embeddings_map = {}\n","for i, row in df_reviewed_products.iterrows():\n","    product_id = row['asin']\n","    reviewed_products_embeddings_map[product_id] = all_product_embeddings_map[product_id]\n","    \n","df['product_embs'] = df['asin'].map(reviewed_products_embeddings_map)"]},{"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"markdown","metadata":{},"source":["### DataLoaders for the Pytorch-based model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def build_train_test_data(data):\n","    \"\"\"For each user, we hold the most recent positive interaction of the user\"\"\"\n","    \n","    train_set = []\n","    test_set = []\n","    \n","    for _, group in data.groupby('reviewerID'):\n","        sorted_group = group.sort_values('reviewTime', ascending=False)\n","        test_item = sorted_group.iloc[0]\n","        train_items = sorted_group.iloc[1:]\n","        \n","        train_set.append(train_items)\n","        test_set.append(test_item)\n","    \n","    train_set = pd.concat(train_set)\n","    test_set = pd.concat(test_set)\n","    \n","    return train_set, test_set\n","\n","# def sample_negatives(df, user_id, num_negatives, all_item_ids):\n","#     interacted_items = set(df[df['user_index'] == user_id]['item_index'])\n","    \n","#     non_interacted_items = list(all_item_ids - interacted_items)\n","    \n","#     sampled_negatives = random.sample(non_interacted_items, min(num_negatives, len(non_interacted_items)))\n","    \n","#     return sampled_negatives"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader \n","import numpy as np\n","\n","# class for the dataset\n","class AmazonReviewDataset(Dataset):\n","    def __init__(self, user_ids, product_ids, ratings, users_text_data, products_text_data):\n","        self.user_ids = user_ids\n","        self.product_ids = product_ids\n","        self.ratings = ratings\n","        self.users_text_data = users_text_data\n","        self.products_text_data = products_text_data\n","        \n","    def __len__(self):\n","        return len(self.user_ids)\n","\n","    def __getitem__(self, index):\n","        user_id = self.user_ids[index]\n","        item_id = self.product_ids[index]\n","        rating = self.ratings[index]\n","        users_text_data = self.users_text_data[index]\n","        products_text_data = self.products_text_data[index]\n","        \n","        return user_id, item_id, rating, users_text_data, products_text_data\n","\n","    \n","train_data, val_data = build_train_test_data(df)\n","\n","\n","train_dataset = AmazonReviewDataset(\n","    train_data['user_index'].values,\n","    train_data['item_index'].values,\n","    train_data['overall'].values,\n","    train_data['user_embs'].values,\n","    train_data['product_embs'].values\n","\n",")\n","\n","test_dataset = AmazonReviewDataset(\n","    val_data['user_index'].values,\n","    val_data['item_index'].values,\n","    val_data['overall'].values,\n","    val_data['user_embs'].values,\n","    val_data['product_embs'].values\n",")\n","\n","# neg_dataset = AmazonReviewDataset(\n","#     neg_data['user_index'].values,\n","#     neg_data['item_index'].values,\n","#     neg_data['overall'].values,\n","#     neg_data['user_embs'].values,\n","#     neg_data['product_embs'].values\n","# )"]},{"cell_type":"markdown","metadata":{},"source":["### Quick model test with SVD from scikit-surprise"]},{"cell_type":"markdown","metadata":{},"source":["Quick performance check of SVD by using scikit-surprise:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from surprise import BaselineOnly, Dataset, SVD, Reader, accuracy, Trainset\n","from surprise.model_selection import cross_validate, train_test_split, KFold\n","\n","reader = Reader(rating_scale=(1, 5))\n","\n","data_test = Dataset.load_from_df(df[[\"reviewerID\", \"asin\", \"overall\"]], reader)\n","\n","trainset, testset = train_test_split(data_test, test_size=0.30)\n","\n","kf = KFold(n_splits=5)\n","\n","algo = SVD()\n","\n","for trainset, testset in kf.split(data_test):\n","\n","    algo.fit(trainset)\n","    predictions = algo.test(testset)\n","\n","    accuracy.rmse(predictions, verbose=True)\n","    \n","user_id = 'A1HBTW5M7ZZ9PT'\n","test_user = reviews_df[reviews_df['reviewerID'] == user_id]\n","item_id = 'B00006IEI7'\n","\n","pred = algo.predict(user_id, item_id, r_ui=5, verbose=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Neural Collaborative Filtering"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torch import nn"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class NCF(nn.Module):\n","  def __init__(self, n_users, n_items, emb_dim, text_user_dim, text_item_dim, dropout_rate=0.2) -> None:\n","    super(NCF, self).__init__()\n","    self.min_rating = 1\n","    self.max_rating = 5\n","    self.n_users = n_users\n","    self.n_items = n_items\n","    self.emb_dim = emb_dim\n","    \n","    # GMF embeddings\n","    self.user_embedding_gmf = nn.Embedding(n_users, emb_dim)\n","    self.item_embedding_gmf = nn.Embedding(n_items, emb_dim)\n","    \n","    # MLP embeddings\n","    self.user_embedding_mlp = nn.Embedding(n_users, 512 // 2)\n","    self.item_embedding_mlp = nn.Embedding(n_items, 512 // 2)\n","    \n","    # text embeddings\n","    self.user_text_layer = nn.Linear(text_user_dim, 512 // 2)\n","    self.item_text_layer = nn.Linear(text_item_dim, 512 // 2)\n","    \n","    # MLP layers\n","    self.fc_layers = nn.Sequential(\n","      nn.Linear(2448, 1024),\n","      nn.ReLU(),\n","      nn.Dropout(p=dropout_rate),\n","      nn.BatchNorm1d(1024),\n","      \n","      nn.Linear(1024, 512),\n","      nn.ReLU(),\n","      nn.Dropout(p=dropout_rate),\n","      nn.BatchNorm1d(512),\n","      \n","      nn.Linear(512 * 2, 256),\n","      nn.ReLU(),\n","      nn.Dropout(p=dropout_rate),\n","      nn.BatchNorm1d(256),\n","      \n","      nn.Linear(256, 128),\n","      nn.ReLU(),\n","      nn.Dropout(p=dropout_rate),\n","      nn.BatchNorm1d(128),\n","      \n","      nn.Linear(128, 64),\n","      nn.ReLU()\n","    )\n","    \n","    self.output_layer = nn.Linear(emb_dim + 64, 1)\n","    \n","    # Initialize weights\n","    self._init_embs_weight_()\n","        \n","  def _init_embs_weight_(self):\n","    nn.init.normal_(self.user_embedding_gmf.weight, std=0.01)\n","    nn.init.normal_(self.item_embedding_gmf.weight, std=0.01)\n","    nn.init.normal_(self.user_embedding_mlp.weight, std=0.01)\n","    nn.init.normal_(self.item_embedding_mlp.weight, std=0.01)\n","    \n","  def forward(self, user_ids, item_ids, user_texts, item_texts):\n","    # GMF part\n","    user_gmf = self.user_embedding_gmf(user_ids)\n","    item_gmf = self.item_embedding_gmf(item_ids)\n","    gmf_output = user_gmf * item_gmf\n","    \n","    # MLP part\n","    user_mlp = self.user_embedding_mlp(user_ids)\n","    item_mlp = self.item_embedding_mlp(item_ids)\n","    user_text_mlp = self.user_text_layer(user_texts)\n","    item_text_mlp = self.item_text_layer(item_texts)\n","    mlp_input = torch.cat([user_mlp, item_mlp, user_text_mlp, item_text_mlp], dim=1)\n","    # mlp_input = torch.cat([user_mlp, item_mlp], dim=1)\n","    mlp_output = self.fc_layers(mlp_input)\n","    \n","    # Concatenate GMF and MLP output\n","    output = self.output_layer(torch.cat([gmf_output, mlp_output], dim=1))\n","    output = output * (self.max_rating - self.min_rating) + self.min_rating\n","    return output"]},{"cell_type":"markdown","metadata":{},"source":["#### Training and evaluating - related functions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train the NCF model\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from tqdm import tqdm\n","from torch import nn\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","class RMSELoss(nn.Module):\n","    def __init__(self, eps=1e-6):\n","        super().__init__()\n","        self.mse = nn.MSELoss()\n","        self.eps = eps\n","        \n","    def forward(self,yhat,y):\n","        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n","        return loss\n","\n","def train_model(model: NCF, train_data: AmazonReviewDataset, val_data: AmazonReviewDataset, loss_func, optimizer, device, num_epochs):\n","    train_loss_history = []\n","    valid_loss_history = []\n","    hit_ratio_avg_history, ndcg_avg_history = [], []\n","    \n","    model.to(device)\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4)\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss_sum = 0\n","\n","        # For each batch\n","        for user_ids, item_ids, rating, u_text_embeds, p_text_embeds in tqdm(train_data, desc='Training...'):\n","            user_ids, item_ids, rating = user_ids.to(device), item_ids.to(device), rating.to(device)\n","            u_text_embeds, p_text_embeds = u_text_embeds.to(device), p_text_embeds.to(device)\n","            \n","            optimizer.zero_grad()\n","            outputs = model(user_ids, item_ids, u_text_embeds, p_text_embeds)\n","            loss = loss_func(outputs.squeeze(), rating.float())\n","            loss.backward()\n","            optimizer.step()\n","            \n","            train_loss_sum += loss.item()\n","\n","        model.eval()\n","        val_loss, _, _ = predict_and_evaluate(model, val_data, loss_func, device)\n","        scheduler.step(val_loss)\n","        \n","        # for both training and validation, the actual train loss\n","        # is the average loss over the entire dataset (the multiple batches)\n","        train_loss = np.sqrt(train_loss_sum / len(train_data))\n","        train_loss_history.append(train_loss)\n","        valid_loss_history.append(val_loss)\n","        \n","        # HR@k and NDCG@K\n","#         hit_ratio_avg, ndcg_avg = evaluate(model, val_data, df['asin'].nunique(), device)\n","#         hit_ratio_avg_history.append(hit_ratio_avg)\n","#         ndcg_avg_history.append(ndcg_avg)\n","        \n","#         print(f'Epoch {epoch+1}: train loss = {train_loss:.4f}, HR@k = {hit_ratio_avg:.4f}, NDCG@k = {ndcg_avg:.4f}')\n","        print(f'Epoch {epoch+1}: train loss = {train_loss:.4f}, val loss = {val_loss:.4f}')\n","    \n","    return train_loss_history, valid_loss_history\n","\n","# def evaluate(model, val_data, item_count, device):\n","#     model.eval()\n","#     hit_ratio_sum = 0.0\n","#     ndcg_sum = 0.0\n","    \n","#     with torch.no_grad():\n","#         for user_id, item_id, rating, u_text_emb, p_text_emb in test_loader:\n","#             user_id, item_id, rating = user_id.to(device), item_id.to(device), rating.to(device)\n","#             u_text_emb, p_text_emb = u_text_emb.to(device), p_text_emb.to(device) \n","            \n","#             # Randomly sample 100 items not interacted by the user\n","#             non_interacted_items = torch.randint(0, item_count, (100,)).to(device)\n","#             non_interacted_items = non_interacted_items[non_interacted_items != item_id]\n","            \n","#             # Rank the test item among the sampled items\n","#             test_item_score = model(user_id, item_id, u_text_emb, p_text_emb)\n","#             non_interacted_scores = model(user_id.expand(non_interacted_items.size(0)), non_interacted_items)\n","#             scores = torch.cat((test_item_score, non_interacted_scores))\n","            \n","#             # Calculate HR and NDCG\n","#             _, indices = torch.topk(scores, k=10)\n","#             hit_ratio = (indices == 0).float().mean().item()\n","#             ndcg = 1.0 / torch.log2(torch.where(indices == 0)[0][0].float() + 2).item()\n","            \n","#             hit_ratio_sum += hit_ratio\n","#             ndcg_sum += ndcg\n","    \n","#     hit_ratio_avg = hit_ratio_sum / len(test_loader)\n","#     ndcg_avg = ndcg_sum / len(test_loader)\n","    \n","#     return hit_ratio_avg, ndcg_avg\n","        \n","# Evaluate the NCF model\n","def predict_and_evaluate(model, data, loss_func, device):\n","    predictions = []\n","    true_ratings = []\n","    loss_sum = 0.0\n","    \n","    with torch.no_grad():\n","        for user_ids, item_ids, rating, u_text_embeds, p_text_embeds in tqdm(data, desc='Evaluating...'):\n","            user_ids = user_ids.to(device)\n","            item_ids = item_ids.to(device)\n","            rating = rating.to(device)\n","            u_text_embeds = u_text_embeds.to(device)\n","            p_text_embeds = p_text_embeds.to(device)\n","            \n","            outputs = model(user_ids, item_ids, u_text_embeds, p_text_embeds)\n","            predictions.extend(outputs.cpu().numpy())\n","            true_ratings.extend(rating.cpu().numpy())\n","            \n","            loss_sum += loss_func(outputs.squeeze(), rating.float()).item()\n","\n","    rmse = np.sqrt(loss_sum / len(data))\n","    mse = mean_squared_error(true_ratings, predictions)\n","    mae = mean_absolute_error(true_ratings, predictions)\n","    return rmse, mse, mae"]},{"cell_type":"markdown","metadata":{},"source":["#### Model training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch.optim as optim\n","import torch.nn as nn\n","torch.cuda.empty_cache()\n","\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n","\n","# Create the NCF model\n","num_users = len(user_id_map)\n","num_items = len(product_id_map)\n","emb_dim = 300\n","droput_rate_fc = 0.2\n","text_embedding_dims = [emb.shape[1] for emb in [reviews_text_embs, products_text_embs]]\n","\n","model = NCF(\n","  num_users, num_items, emb_dim, 200, 200, droput_rate_fc).to(device)\n","\n","# Define loss function and optimizer\n","loss_func = RMSELoss().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","\n","# Train the model\n","num_epochs = 10\n","print('Loss criterion: RMSE\\n')\n","train_loss_history, val_loss_history = train_model(model, train_loader, test_loader, loss_func, optimizer, device, num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def plot_accuracy(train_acc_history, val_acc_history, loss_name='RMSE'):\n","  df = pd.DataFrame({'Epoch': range(1, len(train_acc_history)+1),\n","             'Train Accuracy': train_acc_history,\n","             'Validation Accuracy': val_acc_history})\n","\n","  plt.figure(figsize=(10, 6))\n","  sns.lineplot(data=df, x='Epoch', y='Train Accuracy', label=f'Train loss ({loss_name})')\n","  sns.lineplot(data=df, x='Epoch', y='Validation Accuracy', label=f'Validation loss ({loss_name})')\n","  plt.xlabel('Epoch')\n","  plt.ylabel(loss_name)\n","  plt.title(f'Training and Validation loss {loss_name}')\n","  plt.legend()\n","  plt.show()\n","  \n","plot_accuracy(train_loss_history, val_loss_history, 'RMSE')"]},{"cell_type":"markdown","metadata":{},"source":["#### Model test with single predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def predict_score(user_id, item_id, model, user_mapping, item_mapping, device):\n","    # Convert user ID and item ID to their corresponding indices\n","    user_index = user_mapping[user_id]\n","    item_index = item_mapping[item_id]\n","\n","    # Convert indices to tensors\n","    user_tensor = torch.tensor([user_index], dtype=torch.long).to(device)\n","    item_tensor = torch.tensor([item_index], dtype=torch.long).to(device)\n","\n","    # Get the predicted score from the model\n","    with torch.no_grad():\n","        model.eval()\n","        score = model(user_tensor, item_tensor, user_embeddings_map[user_id].to(device), all_product_embeddings_map[item_id].to(device)).item()\n","\n","    return score\n","\n","item_id = 'B002H9Y8X2'\n","user_id = 'A2TUPLNRVDE0P6'\n","print(f'Predicted rating for user {user_id} and item {item_id}: {predict_score(user_id, item_id, model, user_id_map, product_id_map, device)}')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4594666,"sourceId":7838159,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
