{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIRS: Neural-based recommender system with a focus on interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import random\n",
    "import sklearn\n",
    "import torch\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    # Seed the random number generator\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Seed NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Seed scikit-learn\n",
    "    sklearn.utils.check_random_state(seed)\n",
    "\n",
    "    # Seed PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # Set pandas options\n",
    "    pd.set_option('display.max_columns', None)  # Display all columns in pandas DataFrames\n",
    "    pd.set_option('display.max_rows', None)  # Display all rows in pandas DataFrames\n",
    "    pd.set_option('display.width', None)  # Disable column width restriction\n",
    "    pd.set_option('display.expand_frame_repr', False)  # Prevent line wrapping in pandas DataFrames\n",
    "\n",
    "\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset from json\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'r')\n",
    "  for l in g:\n",
    "    yield json.loads(l.strip())\n",
    "    \n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "    \n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "\n",
    "def sample_data(reviews_df, products_df, min_reviews_count=10, max_users=1000, frac_sampled_products=0.1):\n",
    "    # Sample a subset of users based on the number of reviews they have\n",
    "    user_reviews_count = reviews_df['reviewerID'].value_counts()\n",
    "    selected_users = user_reviews_count[user_reviews_count >= min_reviews_count].index[:max_users]\n",
    "    reviews_subset: pd.DataFrame = reviews_df[reviews_df['reviewerID'].isin(selected_users)]\n",
    "\n",
    "    # Sample a subset of products based on popularity or ratings\n",
    "    # You can use salesRank or overall ratings for this purpose\n",
    "    sampled_products: pd.DataFrame = products_df.sample(frac=frac_sampled_products, random_state=42)\n",
    "\n",
    "    return reviews_subset, sampled_products\n",
    "\n",
    "def count_nan_values(df):\n",
    "    nan_counts = df.isna().sum()\n",
    "    return nan_counts[nan_counts > 0]\n",
    "\n",
    "def count_empty_strings(df):\n",
    "    empty_string_counts = (df == '').sum()\n",
    "    return empty_string_counts[empty_string_counts > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = getDF('data/Office_Products_5.json.gz').drop(['verified', 'reviewTime', 'style', 'image', 'vote'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800357, 7)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products = getDF('data/meta_Office_Products.json.gz').drop([\"imageURL\", \"imageURLHighRes\", 'tech1', 'tech2', 'fit'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewerName    140\n",
       "reviewText      213\n",
       "summary         129\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_nan_values(df_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>also_buy</th>\n",
       "      <th>brand</th>\n",
       "      <th>feature</th>\n",
       "      <th>rank</th>\n",
       "      <th>also_view</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>similar_item</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>asin</th>\n",
       "      <th>details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Office Products, Office &amp; School Supplies, Ed...</td>\n",
       "      <td>[Sequential Spelling is based on the classic O...</td>\n",
       "      <td>Sequential Spelling Level 1 Bundle with Studen...</td>\n",
       "      <td>[]</td>\n",
       "      <td>STL Distributors</td>\n",
       "      <td>[]</td>\n",
       "      <td>[&gt;#439,654 in Office Products (See top 100), &gt;...</td>\n",
       "      <td>[1935943065, 1935943073, B00IJH9Q4M, 002115021...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>class=\"a-bordered a-horizontal-stripes  a-spa...</td>\n",
       "      <td>August 15, 2014</td>\n",
       "      <td>$32.90</td>\n",
       "      <td>0012624861</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Office Products, Office &amp;amp; School Supplies...</td>\n",
       "      <td>[Unusual book, , ]</td>\n",
       "      <td>Mathematics, Applications and Concepts, Course...</td>\n",
       "      <td>[]</td>\n",
       "      <td>bailey</td>\n",
       "      <td>[]</td>\n",
       "      <td>3,839,628 in Books (</td>\n",
       "      <td>[]</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>$8.62</td>\n",
       "      <td>0078652669</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>[Pearson MyHistoryLab Online Access Code for A...</td>\n",
       "      <td>Pearson MyHistoryLab Online Access Code for Am...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Pearson MyHistoryLab</td>\n",
       "      <td>[Pearson MyHistoryLab Online Access Code for A...</td>\n",
       "      <td>[&gt;#1,925,354 in Office Products (See top 100)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Office Products</td>\n",
       "      <td></td>\n",
       "      <td>June 21, 2012</td>\n",
       "      <td>$0.99</td>\n",
       "      <td>0136039847</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Office Products, Office &amp; School Supplies, Ed...</td>\n",
       "      <td>[Corduroy the bear goes to the launderette wit...</td>\n",
       "      <td>A Pocket for Corduroy</td>\n",
       "      <td>[0140501738, 0448421917, 0670063428, 042528875...</td>\n",
       "      <td>Ingram Book &amp; Distributor</td>\n",
       "      <td>[9780140503524]</td>\n",
       "      <td>[&gt;#422,894 in Office Products (See top 100), &gt;...</td>\n",
       "      <td>[0140501738]</td>\n",
       "      <td>Office Products</td>\n",
       "      <td></td>\n",
       "      <td>September 14, 2006</td>\n",
       "      <td>$0.95</td>\n",
       "      <td>0140503528</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Office Products, Office &amp; School Supplies, Ed...</td>\n",
       "      <td>[&lt;div class=\"aplus\"&gt; &lt;div class=\"leftImage\" st...</td>\n",
       "      <td>Social Entrepreneurship: What Everyone Needs t...</td>\n",
       "      <td>[0195334760, 1613630328, 1422104060, 158648956...</td>\n",
       "      <td>Visit Amazon's David Bornstein Page</td>\n",
       "      <td>[]</td>\n",
       "      <td>110,732 in Books (</td>\n",
       "      <td>[0195334760, 1586489569, 1613630328, 142210406...</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0195396332</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            category                                        description                                              title                                           also_buy                                brand                                            feature                                               rank                                          also_view         main_cat                                       similar_item                date   price        asin details\n",
       "0  [Office Products, Office & School Supplies, Ed...  [Sequential Spelling is based on the classic O...  Sequential Spelling Level 1 Bundle with Studen...                                                 []                     STL Distributors                                                 []  [>#439,654 in Office Products (See top 100), >...  [1935943065, 1935943073, B00IJH9Q4M, 002115021...  Office Products   class=\"a-bordered a-horizontal-stripes  a-spa...     August 15, 2014  $32.90  0012624861     NaN\n",
       "1  [Office Products, Office &amp; School Supplies...                                 [Unusual book, , ]  Mathematics, Applications and Concepts, Course...                                                 []                               bailey                                                 []                               3,839,628 in Books (                                                 []            Books                                                                          $8.62  0078652669     NaN\n",
       "2                                                 []  [Pearson MyHistoryLab Online Access Code for A...  Pearson MyHistoryLab Online Access Code for Am...                                                 []                 Pearson MyHistoryLab  [Pearson MyHistoryLab Online Access Code for A...     [>#1,925,354 in Office Products (See top 100)]                                                 []  Office Products                                                          June 21, 2012   $0.99  0136039847     NaN\n",
       "3  [Office Products, Office & School Supplies, Ed...  [Corduroy the bear goes to the launderette wit...                              A Pocket for Corduroy  [0140501738, 0448421917, 0670063428, 042528875...            Ingram Book & Distributor                                    [9780140503524]  [>#422,894 in Office Products (See top 100), >...                                       [0140501738]  Office Products                                                     September 14, 2006   $0.95  0140503528     NaN\n",
       "4  [Office Products, Office & School Supplies, Ed...  [<div class=\"aplus\"> <div class=\"leftImage\" st...  Social Entrepreneurship: What Everyone Needs t...  [0195334760, 1613630328, 1422104060, 158648956...  Visit Amazon's David Bornstein Page                                                 []                                 110,732 in Books (  [0195334760, 1586489569, 1613630328, 142210406...            Books                                                                                 0195396332     NaN"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "details    7147\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_nan_values(df_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                3\n",
       "brand             4865\n",
       "main_cat          1983\n",
       "similar_item    194977\n",
       "date             39165\n",
       "price           142745\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_empty_strings(df_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(315458, 14)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_sampled, df_products_sampled = sample_data(df_reviews, df_products, min_reviews_count=10, frac_sampled_products=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the sampled reviews dataset: (45779, 7)\n",
      "Shape of the sampled products dataset: (31546, 14)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of the sampled reviews dataset: {df_reviews_sampled.shape}')\n",
    "print(f'Shape of the sampled products dataset: {df_products_sampled.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sampled_data(reviews_df, products_df, reviews_file, products_file):\n",
    "    reviews_df.to_csv(reviews_file, index=False)\n",
    "    products_df.to_csv(products_file, index=False)\n",
    "\n",
    "save_sampled_data(df_reviews_sampled, df_products_sampled, 'data/reviews_sampled.csv', 'data/products_sampled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test reading from the csv\n",
    "df_reviews_sampled = pd.read_csv('data/reviews_sampled.csv')\n",
    "df_products_sampled = pd.read_csv('data/products_sampled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(html)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m html \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m html \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m html \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m html \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m html \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m html \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m html \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m html \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m html \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m html \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNA\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m html \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mna\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     10\u001b[0m     \n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Parse the HTML\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Find all table rows ('tr' elements)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     rows \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/hugo-work/lib/python3.11/site-packages/bs4/__init__.py:315\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(markup, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):        \u001b[38;5;66;03m# It's a file-type object.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     markup \u001b[38;5;241m=\u001b[39m markup\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    316\u001b[0m         (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[1;32m    318\u001b[0m ):\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# Issue warnings for a couple beginner problems\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# involving passing non-markup to Beautiful Soup.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;66;03m# Beautiful Soup will still parse the input as markup,\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m# since that is sometimes the intended behavior.\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markup_is_url(markup):\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markup_resembles_filename(markup)                \n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "html = df_products.iloc[0]['tech2']\n",
    "print(html)\n",
    "\n",
    "if html is not None and html != '' and html != '[]' and html != '{}' \\\n",
    "    and html != 'No' and html != 'no' and html != 'N/A' and html != 'none' \\\n",
    "    and html != 'None' and html != 'NA' and html != 'na':\n",
    "    \n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find all table rows ('tr' elements)\n",
    "    rows = soup.find_all('tr')\n",
    "\n",
    "    # Extract the key-value pairs\n",
    "    data = {}\n",
    "    for row in rows:\n",
    "        th = row.find('th')  # Find the table header ('th') element\n",
    "        td = row.find('td')  # Find the table data ('td') element\n",
    "        \n",
    "        if th and td:\n",
    "            key = th.text.strip()  # Extract the key and remove leading/trailing whitespace\n",
    "            value = td.text.strip()  # Extract the value and remove leading/trailing whitespace\n",
    "            data[key] = value  # Store the key-value pair in the data dictionary\n",
    "\n",
    "    # Print the extracted data\n",
    "    for key, value in data.items():\n",
    "        print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>description</th>\n",
       "      <th>fit</th>\n",
       "      <th>title</th>\n",
       "      <th>also_buy</th>\n",
       "      <th>tech2</th>\n",
       "      <th>brand</th>\n",
       "      <th>feature</th>\n",
       "      <th>rank</th>\n",
       "      <th>also_view</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>similar_item</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>asin</th>\n",
       "      <th>details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Office Products, Office &amp; School Supplies, Ed...</td>\n",
       "      <td>[Sequential Spelling is based on the classic O...</td>\n",
       "      <td></td>\n",
       "      <td>Sequential Spelling Level 1 Bundle with Studen...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>STL Distributors</td>\n",
       "      <td>[]</td>\n",
       "      <td>[&gt;#439,654 in Office Products (See top 100), &gt;...</td>\n",
       "      <td>[1935943065, 1935943073, B00IJH9Q4M, 002115021...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>class=\"a-bordered a-horizontal-stripes  a-spa...</td>\n",
       "      <td>August 15, 2014</td>\n",
       "      <td>$32.90</td>\n",
       "      <td>0012624861</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>[Pearson MyHistoryLab Online Access Code for A...</td>\n",
       "      <td></td>\n",
       "      <td>Pearson MyHistoryLab Online Access Code for Am...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Pearson MyHistoryLab</td>\n",
       "      <td>[Pearson MyHistoryLab Online Access Code for A...</td>\n",
       "      <td>[&gt;#1,925,354 in Office Products (See top 100)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Office Products</td>\n",
       "      <td></td>\n",
       "      <td>June 21, 2012</td>\n",
       "      <td>$0.99</td>\n",
       "      <td>0136039847</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Office Products, Office &amp; School Supplies, Ed...</td>\n",
       "      <td>[Corduroy the bear goes to the launderette wit...</td>\n",
       "      <td></td>\n",
       "      <td>A Pocket for Corduroy</td>\n",
       "      <td>[0140501738, 0448421917, 0670063428, 042528875...</td>\n",
       "      <td></td>\n",
       "      <td>Ingram Book &amp; Distributor</td>\n",
       "      <td>[9780140503524]</td>\n",
       "      <td>[&gt;#422,894 in Office Products (See top 100), &gt;...</td>\n",
       "      <td>[0140501738]</td>\n",
       "      <td>Office Products</td>\n",
       "      <td></td>\n",
       "      <td>September 14, 2006</td>\n",
       "      <td>$0.95</td>\n",
       "      <td>0140503528</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Office Products, Office &amp; School Supplies, Bo...</td>\n",
       "      <td>[A good helper to help you record your reading...</td>\n",
       "      <td></td>\n",
       "      <td>EKLOEN Mixed Designs of Antiqued Bronze Colour...</td>\n",
       "      <td>[B00BLY6POE, B00G8WV5U8, 0307591662, B019XJZHQ...</td>\n",
       "      <td></td>\n",
       "      <td>EKLOEN</td>\n",
       "      <td>[It is not only the bookmarks, but also art, I...</td>\n",
       "      <td>[&gt;#43,748 in Office Products (See top 100), &gt;#...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>class=\"a-bordered a-horizontal-stripes  a-spa...</td>\n",
       "      <td>November 8, 2015</td>\n",
       "      <td>$17.50</td>\n",
       "      <td>0245109919</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[Office Products, Office &amp; School Supplies, Fo...</td>\n",
       "      <td>[Unique DesignNovel Appearance&lt;br&gt; Electroplat...</td>\n",
       "      <td></td>\n",
       "      <td>Jzcky Shzrp Unique Appearance Premium Quality ...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Jzcky Shzrp</td>\n",
       "      <td>[Unique Appearance Design, Material:Eco-friend...</td>\n",
       "      <td>[&gt;#835,737 in Office Products (See top 100), &gt;...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Office Products</td>\n",
       "      <td></td>\n",
       "      <td>February 19, 2016</td>\n",
       "      <td></td>\n",
       "      <td>0357051378</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             category  \\\n",
       "0   [Office Products, Office & School Supplies, Ed...   \n",
       "2                                                  []   \n",
       "3   [Office Products, Office & School Supplies, Ed...   \n",
       "5   [Office Products, Office & School Supplies, Bo...   \n",
       "23  [Office Products, Office & School Supplies, Fo...   \n",
       "\n",
       "                                          description fit  \\\n",
       "0   [Sequential Spelling is based on the classic O...       \n",
       "2   [Pearson MyHistoryLab Online Access Code for A...       \n",
       "3   [Corduroy the bear goes to the launderette wit...       \n",
       "5   [A good helper to help you record your reading...       \n",
       "23  [Unique DesignNovel Appearance<br> Electroplat...       \n",
       "\n",
       "                                                title  \\\n",
       "0   Sequential Spelling Level 1 Bundle with Studen...   \n",
       "2   Pearson MyHistoryLab Online Access Code for Am...   \n",
       "3                               A Pocket for Corduroy   \n",
       "5   EKLOEN Mixed Designs of Antiqued Bronze Colour...   \n",
       "23  Jzcky Shzrp Unique Appearance Premium Quality ...   \n",
       "\n",
       "                                             also_buy tech2  \\\n",
       "0                                                  []         \n",
       "2                                                  []         \n",
       "3   [0140501738, 0448421917, 0670063428, 042528875...         \n",
       "5   [B00BLY6POE, B00G8WV5U8, 0307591662, B019XJZHQ...         \n",
       "23                                                 []         \n",
       "\n",
       "                        brand  \\\n",
       "0            STL Distributors   \n",
       "2        Pearson MyHistoryLab   \n",
       "3   Ingram Book & Distributor   \n",
       "5                      EKLOEN   \n",
       "23                Jzcky Shzrp   \n",
       "\n",
       "                                              feature  \\\n",
       "0                                                  []   \n",
       "2   [Pearson MyHistoryLab Online Access Code for A...   \n",
       "3                                     [9780140503524]   \n",
       "5   [It is not only the bookmarks, but also art, I...   \n",
       "23  [Unique Appearance Design, Material:Eco-friend...   \n",
       "\n",
       "                                                 rank  \\\n",
       "0   [>#439,654 in Office Products (See top 100), >...   \n",
       "2      [>#1,925,354 in Office Products (See top 100)]   \n",
       "3   [>#422,894 in Office Products (See top 100), >...   \n",
       "5   [>#43,748 in Office Products (See top 100), >#...   \n",
       "23  [>#835,737 in Office Products (See top 100), >...   \n",
       "\n",
       "                                            also_view         main_cat  \\\n",
       "0   [1935943065, 1935943073, B00IJH9Q4M, 002115021...  Office Products   \n",
       "2                                                  []  Office Products   \n",
       "3                                        [0140501738]  Office Products   \n",
       "5                                                  []  Office Products   \n",
       "23                                                 []  Office Products   \n",
       "\n",
       "                                         similar_item                date  \\\n",
       "0    class=\"a-bordered a-horizontal-stripes  a-spa...     August 15, 2014   \n",
       "2                                                           June 21, 2012   \n",
       "3                                                      September 14, 2006   \n",
       "5    class=\"a-bordered a-horizontal-stripes  a-spa...    November 8, 2015   \n",
       "23                                                      February 19, 2016   \n",
       "\n",
       "     price        asin details  \n",
       "0   $32.90  0012624861     NaN  \n",
       "2    $0.99  0136039847     NaN  \n",
       "3    $0.95  0140503528     NaN  \n",
       "5   $17.50  0245109919     NaN  \n",
       "23          0357051378     NaN  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_products_filtered = df_products[df_products['main_cat'] == 'Office Products']\n",
    "df_products_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'user_reviews.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the user reviews DataFrame\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m reviews_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_reviews.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the products dataset DataFrame\u001b[39;00m\n\u001b[1;32m      5\u001b[0m products_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproducts.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/hugo-work/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hugo-work/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/hugo-work/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hugo-work/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/hugo-work/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'user_reviews.csv'"
     ]
    }
   ],
   "source": [
    "# Load the user reviews DataFrame\n",
    "reviews_df = pd.read_csv('user_reviews.csv')\n",
    "\n",
    "# Load the products dataset DataFrame\n",
    "products_df = pd.read_csv('products.csv')\n",
    "\n",
    "def get_user_reviews(user_id):\n",
    "    # Filter the reviews DataFrame for a specific user\n",
    "    user_reviews = reviews_df[reviews_df['user_id'] == user_id]\n",
    "\n",
    "    # Join the reviews DataFrame with the products DataFrame using 'asin' as the key\n",
    "    user_reviews = user_reviews.merge(products_df, on='asin')\n",
    "\n",
    "    return user_reviews[['asin', 'product_name', 'review']]\n",
    "\n",
    "# Example usage\n",
    "user_id = '1234'\n",
    "user_reviews = get_user_reviews(user_id)\n",
    "print(f\"User {user_id} has reviewed the following products:\")\n",
    "print(user_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [self._preprocess(text) for text in X]\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        # Lowercasing\n",
    "        text = text.lower()\n",
    "        # Remove accented characters\n",
    "        text = unidecode(text)\n",
    "        \n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # remove punctuation\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "        \n",
    "        # remove double spaces\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        \n",
    "        # Tokenize text\n",
    "        words = word_tokenize(text)\n",
    "        # Remove stopwords and lemmatize\n",
    "        words = [self.lemmatizer.lemmatize(\n",
    "            word) for word in words if word not in self.stop_words]\n",
    "        \n",
    "        return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800089, 155949)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugo/anaconda3/envs/design-project/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/var/folders/0v/j168s2s91bxf0vr_r7rn3m_c0000gn/T/ipykernel_48881/3048727505.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_users['cluster'] = clusters\n"
     ]
    }
   ],
   "source": [
    "# 5 clusters built with the text features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocessor = Pipeline([\n",
    "  ('text_preprocessor', TextPreprocessor()),\n",
    "  ('vectorizer', TfidfVectorizer()),\n",
    "])\n",
    "\n",
    "# Convert text features to numerical representations using TF-IDF vectorization\n",
    "df_users = df\n",
    "#clean rows where reviewText is NaN\n",
    "df_users = df_users.dropna(subset=['reviewText'])\n",
    "\n",
    "X = preprocessor.fit_transform(df_users['reviewText'])\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# Calculate cosine similarity matrix\n",
    "#cosine_sim = cosine_similarity(X)\n",
    "\n",
    "# Apply K-means clustering\n",
    "k = 5  # Number of clusters\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "df_users['cluster'] = clusters\n",
    "\n",
    "# Print the cluster assignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             reviewerName  cluster  \\\n",
      "0             cotton clay        2   \n",
      "1                emankcin        2   \n",
      "2           Starbucks Fan        2   \n",
      "3        Caitlyn Jacobson        2   \n",
      "4                E. Ervin        2   \n",
      "...                   ...      ...   \n",
      "800352               Anky        2   \n",
      "800353                 DM        0   \n",
      "800354  Verdant Treasures        2   \n",
      "800355              C. F.        0   \n",
      "800356     Tegan M. Reyes        2   \n",
      "\n",
      "                                               reviewText  \n",
      "0       kids like story BUT while i really wanted a bo...  \n",
      "1       Bought this used and it came in great conditio...  \n",
      "2       Every story and book about Corduroy is Fantast...  \n",
      "3       I purchased this book for my first grade class...  \n",
      "4       Having spent numerous years in an elementary s...  \n",
      "...                                                   ...  \n",
      "800352              Delivered on time and is as expected.  \n",
      "800353                                      worked great.  \n",
      "800354  I used to score free UPS 4x6 labels.  The blac...  \n",
      "800355                                              great  \n",
      "800356  Struggled finding a sharpener and they didn't ...  \n",
      "\n",
      "[800089 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_users[['reviewerName', 'cluster', 'reviewText']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 example:\n",
      "        reviewerName  cluster                    reviewText\n",
      "44   Jason W. Bishop        0                Great product.\n",
      "138          Sue Kim        0              great condition.\n",
      "159  Amazon Customer        0                 great product\n",
      "176       T. Darling        0                        Great!\n",
      "188         lawrence        0  Great product for the price.\n",
      "\n",
      "\n",
      "Cluster 1 example:\n",
      "       reviewerName  cluster                                 reviewText\n",
      "29       BRANDON K.        1  Good product, does what it's supposed to.\n",
      "115  rose ann ramos        1                               good product\n",
      "136       Yesha Luo        1                                       good\n",
      "153   terry lasyone        1                               Good product\n",
      "192          Rafael        1                                       good\n",
      "\n",
      "\n",
      "Cluster 2 example:\n",
      "       reviewerName  cluster  \\\n",
      "0       cotton clay        2   \n",
      "1          emankcin        2   \n",
      "2     Starbucks Fan        2   \n",
      "3  Caitlyn Jacobson        2   \n",
      "4          E. Ervin        2   \n",
      "\n",
      "                                          reviewText  \n",
      "0  kids like story BUT while i really wanted a bo...  \n",
      "1  Bought this used and it came in great conditio...  \n",
      "2  Every story and book about Corduroy is Fantast...  \n",
      "3  I purchased this book for my first grade class...  \n",
      "4  Having spent numerous years in an elementary s...  \n",
      "\n",
      "\n",
      "Cluster 3 example:\n",
      "       reviewerName  cluster  \\\n",
      "22          A. Fogg        3   \n",
      "122      Beverly B.        3   \n",
      "132   Sharon Downey        3   \n",
      "322         sjf1218        3   \n",
      "358  Mohanram Jewan        3   \n",
      "\n",
      "                                            reviewText  \n",
      "22   My youngest daughter picked this out and she e...  \n",
      "122                                    It is very nice  \n",
      "132  Great fitting and well worth the money.\\nWell ...  \n",
      "322                                               nice  \n",
      "358                                              Nice!  \n",
      "\n",
      "\n",
      "Cluster 4 example:\n",
      "       reviewerName  cluster  \\\n",
      "6         Luci Furr        4   \n",
      "87   Nicole Swavely        4   \n",
      "117      Very Happy        4   \n",
      "142           Amber        4   \n",
      "209          CARMAN        4   \n",
      "\n",
      "                                            reviewText  \n",
      "6                                       Love this book  \n",
      "87                                          son loves!  \n",
      "117  I love it, I love it. perfect fit for my large...  \n",
      "142                                            love it  \n",
      "209                                           Love it.  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show examples of each cluster\n",
    "for i in range(k):\n",
    "    print(f\"Cluster {i} example:\")\n",
    "    print(df_users[df_users['cluster'] == i].head(5)[['reviewerName', 'cluster', 'reviewText']])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "\n",
    "def get_bert_embeddings(sentences, batch_size=32):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    progress_bar = tqdm(range(0, len(sentences), batch_size), desc=\"Generating Embeddings\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in progress_bar:\n",
    "            batch_sentences = sentences[i:i+batch_size]\n",
    "            inputs = tokenizer(batch_sentences, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :]  # Use the embedding of the [CLS] token\n",
    "            embeddings.append(batch_embeddings.cpu())  # Move embeddings back to CPU if needed\n",
    "\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 2502/2502 [12:35<00:00,  3.31it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4292, -0.3136,  0.0209,  ..., -0.0545,  0.2543,  0.1028],\n",
      "        [-0.1332, -0.0620,  0.0943,  ..., -0.0172,  0.2085,  0.1851],\n",
      "        [-0.3114, -0.1208,  0.0139,  ..., -0.1258,  0.2320,  0.2977],\n",
      "        ...,\n",
      "        [-0.2222,  0.1290, -0.1111,  ..., -0.0086,  0.4601,  0.3364],\n",
      "        [ 0.0441, -0.0720,  0.0839,  ..., -0.0750,  0.1904,  0.0642],\n",
      "        [-0.3814, -0.2903,  0.0730,  ..., -0.1287, -0.0366,  0.1214]])\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame column to a list of reviews\n",
    "# Ensure all reviews are strings; this also handles NaN values gracefully\n",
    "df_reviews['summary'] = df_reviews['summary'].dropna().astype(str)\n",
    "#turn all elements to string\n",
    "df_reviews['summary'] = df_reviews['summary'].astype(str)\n",
    "\n",
    "\n",
    "#reviews = 1% of reviews\n",
    "reviews = df_reviews['summary'][:int(len(df_reviews['summary'])* 0.1)]\n",
    "\n",
    "#print(reviews)\n",
    "\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "preprocessed_reviews =  preprocessor.fit_transform(reviews)\n",
    "\n",
    "#print(preprocessed_reviews)\n",
    "print(len(preprocessed_reviews))\n",
    "# Generate BERT embeddings\n",
    "\n",
    "#show progress bar\n",
    "\n",
    "embeddings = get_bert_embeddings(preprocessed_reviews)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/j168s2s91bxf0vr_r7rn3m_c0000gn/T/ipykernel_17121/152171155.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_products_filtered['description'] = df_products_filtered['description'].astype(str)\n",
      "/var/folders/0v/j168s2s91bxf0vr_r7rn3m_c0000gn/T/ipykernel_17121/152171155.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_products_filtered['description'] = df_products_filtered['description'].str[1:-1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     'Sequential Spelling is based on the classic O...\n",
       "2     'Pearson MyHistoryLab Online Access Code for A...\n",
       "3     'Corduroy the bear goes to the launderette wit...\n",
       "5     'A good helper to help you record your reading...\n",
       "23    'Unique DesignNovel Appearance<br> Electroplat...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a new dataframe where description column is a string instead of a list\n",
    "#remove first and last character of the string\n",
    "df_products_filtered['description'] = df_products_filtered['description'].astype(str)\n",
    "df_products_filtered['description'] = df_products_filtered['description'].str[1:-1]\n",
    "df_products_filtered['description'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 7/7 [00:12<00:00,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3614,  0.0404, -0.0675,  ...,  0.0223,  0.1958,  0.1585],\n",
      "        [-0.3093,  0.0895, -0.2680,  ..., -0.2148,  0.3053,  0.5857],\n",
      "        [-0.4199, -0.2032, -0.0889,  ..., -0.3773,  0.1998, -0.0556],\n",
      "        ...,\n",
      "        [-0.3254, -0.2669,  0.2629,  ..., -0.2515,  0.1685, -0.0035],\n",
      "        [-0.2522, -0.0139, -0.0895,  ..., -0.2284,  0.2832,  0.2796],\n",
      "        [-0.6760, -0.2615,  0.1202,  ..., -0.0768, -0.1347,  0.5695]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prod_descriptions = df_products_filtered['description'][:int(len(df_products_filtered['description'])* 0.1)]\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "preprocessed_descriptions =  preprocessor.fit_transform(prod_descriptions)\n",
    "\n",
    "# Generate BERT embeddings\n",
    "embeddings = get_bert_embeddings(preprocessed_descriptions)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings tensor to numpy array\n",
    "X = embeddings.numpy()\n",
    "\n",
    "# Apply K-means clustering\n",
    "k = 5  # Number of clusters\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "df_users['cluster'] = clusters\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
